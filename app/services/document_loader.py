import os
import glob
from typing import List, Dict, Any
from pathlib import Path

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import (
    PyPDFLoader,
    TextLoader,
    Docx2txtLoader,
    UnstructuredMarkdownLoader
)
from app.core.config import settings


class DocumentLoader:
    """ë¬¸ì„œ ë¡œë” ë° ì²­í‚¹ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=settings.max_chunk_size,
            chunk_overlap=settings.chunk_overlap,
            length_function=len,
            separators=["\n\n", "\n", " ", ""]
        )
    
    def load_documents_from_directory(self, directory: str = "") -> List[Dict[str, Any]]:
        """ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë“  ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³  ì²­í‚¹"""
        if not directory:
            directory = settings.documents_dir
        
        documents = []
        
        # ì§€ì›í•˜ëŠ” íŒŒì¼ í™•ì¥ì
        file_patterns = {
            "*.pdf": PyPDFLoader,
            "*.txt": TextLoader,
            "*.md": TextLoader,
            "*.docx": Docx2txtLoader
        }
        
        for pattern, loader_class in file_patterns.items():
            file_paths = glob.glob(os.path.join(directory, pattern))
            
            for file_path in file_paths:
                try:
                    print(f"ğŸ“„ ë¬¸ì„œ ë¡œë”© ì¤‘: {file_path}")
                    
                    # ë¬¸ì„œ ë¡œë“œ
                    loader = loader_class(file_path)
                    raw_docs = loader.load()
                    
                    # ì²­í‚¹
                    chunks = self.text_splitter.split_documents(raw_docs)
                    
                    # ë©”íƒ€ë°ì´í„° ì¶”ê°€
                    for i, chunk in enumerate(chunks):
                        chunk.metadata.update({
                            "source_file": os.path.basename(file_path),
                            "file_path": file_path,
                            "chunk_index": i,
                            "total_chunks": len(chunks)
                        })
                    
                    documents.extend(chunks)
                    print(f"âœ… {file_path}: {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
                    
                except Exception as e:
                    print(f"âŒ {file_path} ë¡œë”© ì‹¤íŒ¨: {e}")
                    continue
        
        print(f"ğŸ“Š ì´ {len(documents)}ê°œ ë¬¸ì„œ ì²­í¬ ë¡œë“œ ì™„ë£Œ")
        return documents
    
    def load_confluence_documents(self, space_key: str, username: str, api_token: str) -> List[Dict[str, Any]]:
        """Confluenceì—ì„œ ë¬¸ì„œ ë¡œë“œ (í–¥í›„ êµ¬í˜„)"""
        # TODO: Confluence API ì—°ë™ êµ¬í˜„
        print("ğŸ”„ Confluence ì—°ë™ ê¸°ëŠ¥ì€ í–¥í›„ êµ¬í˜„ ì˜ˆì •ì…ë‹ˆë‹¤.")
        return []
    
    def get_document_info(self, directory: str = "") -> Dict[str, Any]:
        """ë¬¸ì„œ ë””ë ‰í† ë¦¬ ì •ë³´ ì¡°íšŒ"""
        if not directory:
            directory = settings.documents_dir
        
        if not os.path.exists(directory):
            return {"error": f"ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {directory}"}
        
        file_info = {}
        total_files = 0
        
        for file_path in glob.glob(os.path.join(directory, "*.*")):
            if file_path.lower().endswith(('.pdf', '.txt', '.md', '.docx')):
                file_size = os.path.getsize(file_path)
                file_info[os.path.basename(file_path)] = {
                    "size": file_size,
                    "path": file_path
                }
                total_files += 1
        
        return {
            "directory": directory,
            "total_files": total_files,
            "files": file_info
        }


# ì „ì—­ ë¬¸ì„œ ë¡œë” ì¸ìŠ¤í„´ìŠ¤
document_loader = DocumentLoader() 